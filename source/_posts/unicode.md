---
title: 计算机文本编码 - Unicode 和 UTF8
date: 2019-05-21 13:47:16
categories:
- 技术
tags:
- 计算机原理
- 编码
---

对于文本文件的编码，时常会有一些误区和不理解，比如：Unicode 是什么编码、字节和编码是什么关系等。
本文将依次渐近的，从 ASCII/ Unicode 介绍到 UTF8/16/32，以及文本二进制存储和 URL 编码/多次编码等场景。

<!-- more -->

## Unicode 

ASCII 是大学计算机教科书入门知识点，如果不联系上码位和 Unicode ，还是不好理解的。知道有这个东西在，但是怎么玩的却不是很清楚。

**ASCII 和 Unicode 都是字符集**，字符集可以看做一个映射表或者大数组，里面的每个节点称为 **码位 / 码点 /Code Point**。
字符集是约定俗成的一套规则矩阵，这套规则矩阵里面的每一行或者每个点（码位），都表示一个特定的符号含义且不可改变。大家都需要遵守这套规则矩阵。

ASCII 并不是表示 0-9 或者 a-z 这些我们肉眼看到的符号，这些符号是对于人类友好的视觉表达。比如 a 这个字母，有不同的字体都可以表达，那些不同的字体表达都不能称为 ASCII。
a 字符或者它的不同字体，是 ASCII 字符集里面某个码位的形象化视觉表达。
汉字/泰文/ emoji 等，也是同理。它们是 Unicode 字符集里面某个码位的表达。

计算机只对码位做存储，具体的展示是应用层来做，emoji 等符号是应用层读取到码位后做的符号映射和视觉显示。切换字体，就是对同一个码位，做不同的视觉表达。

所以，这里很重要的一点，是对 **码位** 的理解。**码位才是每一个字符的互联网流通语言**。
比如 `01000000` 这个 ASCII 码位，表示 `@` 符号，大家脑海里回忆一下，这些年是不是见过很多样式的@符号，但在 ASCII 里面，只有 `01000000` 来表示。

从 ASCII 进化到 Unicode，就是增加码位。这里也对码位做下介绍，方便后面 utf 编码的说明。

ASCII 有 255 个码位，有些表达视觉含义如 a-z，有些是不可见的控制符号等。
二进制是：`00000000` - `11111111`

Unicode 采用平面码位设计，共计 17 个平面，每个平面有 65536 个码位，共计 1114112 个码位。
二进制是：`xx 00000000 00000000` - `xx 11111111 11111111`。xx 表示平面，从 0 - 16，范围是：`00000000` - `00010000`

所有的码位，被称为**字符集**。Unicode 也称为 **Unicode 字符集**，ASCII 同理。

## UTF

UTF 这三个，是字符编码，字符编码是一套算法实现。
它们服务于前面说到的 Unicode 字符集，具体职责是：如何对码位进行压缩，以节省空间。

如果没有 UTF 编码，完全通过码位自身的二进制，也可以进行存储。
只不过这会增加存储空间，比如 😀 这个 emoji 符号对应的码位是：`0x1f600`，二进制存储是：`00000001 11110110 00000000`，共计 3 个字节。

UTF 要做的，是定义一套编码和解码的规则，来对刚才码位的二进制进行压缩。就像图片压缩一样，有不同的压缩方案，效果和像素损失也不一样。
UTF 需要兼顾 **大小** 和 **速度** 两个指标。

这里介绍一下 UTF-8 和 UTF-16。

UTF-8 使用多字节来表达一个 Unicode 码位

```
字节数  Unicode码位    十进制          UTF-8 编码
1      000000-00007F  0-127          0xxxxxxx
2      000080-0007FF  128-2047       110xxxxx 10xxxxxx
3      000800-00FFFF  2048-65535     1110xxxx 10xxxxxx 10xxxxxx
4      010000-10FFFF  65536-1114111  11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
```

这里举几个例子，加深印象：

```
符号  码位                                  字节    UTF-8 编码                                      十六进制
a     00000000 00000000 01100001(0x61)      1     01100001                                        0x61
中     00000000 01001110 00101101(0x4E2D)    3     <1110>0100 <10>111000 <10>101101                0xE4B8AD
😀    00000001 11110110 00000000(0x1F600)   4     <11110>000 <10>011111 <10>011000 <10>000000     0xF09F9880
```

如下图所示：

<img src="https://cdn.jsdelivr.net/gh/yigegongjiang/image_space@main/blog_img/202312241510727.png" width="40%">

UTF-16 是固定编码，要么是 2 字节，要么是 4 字节，根据平面来决定。对于第一个平面均使用 2 字节，其他的使用 4 字节。
但是 UTF-16 有个默认2字节前缀，即最小也是 4 字节。最还是上面的例子：

```
符号  码位                                  字节    UTF-16 编码                             十六进制
a     00000000 00000000 01100001(0x61)      2     00000000 01100001                       0x0061
中     00000000 01001110 00101101(0x4E2D)    2     01001110 00101101                       0x4E2D
😀    00000001 11110110 00000000(0x1F600)   4     11011000 00111101 11011110 00000000     0xD83D 0xDE00
```
如下图所示：

<img src="https://cdn.jsdelivr.net/gh/yigegongjiang/image_space@main/blog_img/202312241530132.png" width="40%">

对比差异如下：
1. UTF-8 占用空间最小，但是解析速度低，因为不能按序解析字节，UTF-8 对字节大小是变化非常大的。
2. UTF-16 占用空间中等，解析速度快。因为更多的码位用 2 个字节表示，字节大小基本固定（少部分字符通过 4 个字节表示），解析非常快。
3. UTF-32 占用空间最大，解析速度最快。一个码位用 4 个字节表示，字节大小全部固定，解析非常快。

## URL

对字符非常敏感的一个环境，就是 URL。URL 对组成其内容的元素要求非常严格，只允许下面这些字符：

> A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y  Z 
> a  b  c  d  e  f  g  h  i  j  k  l  m  n  o  p  q  r  s  t  u  v  w  x  y  z
> 0  1  2  3  4  5  6  7  8  9  
> \-  _  .  ~  

ASCII 字符集也没有全部包含，只有上面这些字符是可以在 url 里面直接使用的。不在这个列表里面的，全部都需要进行**百分号转码**。
百分号转码是 URL 特有的一套规则，默认使用 UTF-8 作为字符编码依据，对编码后的字节产物，进行百分号分割。
百分号转码后还需要还原回去，这套流程分别叫做 **URL Encoding** 和 **URL Decoding**。

所以，对 URL 来说，它使用的编解码是 **URL Encoding** 和 **URL Decoding**，这套编码内部通过百分号拼接 UTF-8 字节来实现。

举个例子：

> `http://www.xx.com/中`
> `http%3a%2f%2fwww.xx.com%2f%e4%b8%ad`

原始 url 里面的 `:`、`/`、`中` 均被转码了。其中 `中` 的 UTF-8 形式是 `0xE4B8AD`，上面有介绍过。这里会通过 % 号进行每个字节分割。

这里有个小的疑惑点，即 `%` 号本身，并不是 URL 允许的字符，但是它可以直接用在 URL 中。举个例子：

> `http://www.xx.com/%`
> `http%3a%2f%2fwww.xx.com%2f%25`

这里可以发现，`%` 本身在 **URL Encoding** 中也会被转成 `%25`，因为它的 UTF-8 字节是 `0x25`。

这也是 **URL Encoding** 规则的定义，通过 UTF-8 编码的字节，需要通过 % 进行字节分割。% 虽然不是 URL 允许的字符，但可以出现在 URL 中。

如此之后，URL 实际上可以被多次 encoding，每次 encoding 的时候 % 都会被换成 `%25`。
对 `%` URL Encoding 5 次之后结果是：`%25252525`。如果需要拿到最开始的 `%` 符号，也同样需要对应 URL Decoding 5 次。

所以开发过程中，Encoding 和 Decoding 的次数需要一致，这个非常重要。否则就拿不到正确的 URL。

## 内存字符编码

通过上面三个 UTF 的编码说明，可以得出，空间最小的是 UTF-8 编码。
所以毋庸置疑，网络传输，磁盘存储，都是用的是 UTF-8 编码。占用空间小，对于网络和磁盘来说，比什么都重要。1M 或者 2M 的数据，感觉无所谓，对于 100G，UTF-8 很可能一下子就能节省 50-80% 的空间。

但是内存里面使用的是什么编码呢？当应用程序把文本读入内存后，数据就在内存里面了。这时候内存中的数据一定也有一种编码格式，否则就是直接存储 Unicode 的码位了。
内存和磁盘文件编码有很大的不同。详见另一篇解读[文件存储差异 - 编码](https://www.yigegongjiang.com/2022/文件存储差异-编码/)

具体来说，到内存中，就是一个可执行程序的运行时中了。
对于普通数字类型，是几个字节就是几个字节，比如 char 是 1 字节。
对于字符串类型，则在 UTF-8 和 UTF-16 之间选择，不同语言使用的不一样，但大多都是使用的 UTF-16，因为解析速度更快。
注意，这是语言级别的差异，而非系统级别的。实际上，内存中对于字符串使用什么编码，对程序的执行本身没有太多影响。就是用 UTF-8，最后输出的二进制文件可能会小一些，但是程序执行上也可能会慢一些，但是不影响最终的程序运行结果。

## 文本文件的识别

> 2023.12.25 更

对于一个文件本身而言，存储的内容都是二进制。虽然可以根据后缀认为某个文件是文本文件，但鉴于后缀也可以更改，所以这并不准确。
还有一种方案是读取文件头的 `magic number`。但这种方案对于识别文件二进制类型比较有用，并不能识别出文本文件。因为文本文件一般都是没有特定的 `magic number` 的。

对于图片/视频等文件，读取它们的二进制内容和读取 `.txt` 等文本文件的技术方案一致。
不过对于图片等二进制，显然无法进行有效的 UTF 解码，所以读取后会是乱码。
对同一个文件进行一致的编码和解码，这样写入的二进制就具有一定的规则。读取的时候按照同样的规则进行解析，当然可以识别出当初写入的内容。

当然，可以对文件的二进制内容全部读出来，然后通过 UTF 进行解码，若能解出来，那就是文本文件。
这是稳定的方案，但具有极大的性能损耗。因为一个图片或者视频，它的二进制内容是非常多的，IO 成本过大。

有一个比较小巧的技术方案，即对文本内容主动进行多个位置的截取解码，以较小的性能开销来对文本文件进行识别。

比如从 文件头 N 偏移的位置截取 10 个字节数据。对这 10 字节进行不同维度的解码。
只要能有一次解出来，说明这 10 字节数据是符合文本编码规范的。（大概率无法一次解出。因为 10 字节里面只有一部分是完整的编码数据，两头很可能是被截断的，无法被解码）
同样的操作可以进行 M 次，需要 M 次全部命中，才能认为当前文件的确是文本文件。合理的设置 M 值，对于真的文本文件会具有很高的识别效率。
但只要有一次不命中，既可以确认当前文件不是文本文件。即非文本文件，可以一次命中，效率极高。

这样，也可以过滤图片文件中插入文本这种操作。

这种方案以读取少量的文本内容和多次匹配作为代价，可以比较稳定的确认当前文件是否是文本文件。

源代码可参考：[HLVFileDump](https://github.com/yigegongjiang/HLVFileDump)

___

体毛具有生长期、休止期。头发的生长期很长，可以持续 2-6 年。腿毛的生长期只有几个月。
生长期结束后就不长了，休止期会退毛，然后毛囊长出新的毛发。重复生长期过程。
所以头发可以很长但是腿毛就很短。
